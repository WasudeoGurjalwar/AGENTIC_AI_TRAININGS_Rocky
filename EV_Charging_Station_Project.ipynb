{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/WasudeoGurjalwar/AGENTIC_AI_TRAININGS_Rocky/blob/main/EV_Charging_Station_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "704de647"
      },
      "source": [
        "# Task\n",
        "Implement a multi-agent EV Charging station information system and booking support support workflow using LangGraph. The workflow should include a Classifier Agent to route queries (Charging Station, Booking , Cafe) to specialized agents (Charging Station, Booking , Cafe). The final responses from the specialized agents should be aggregated into a single answer. Use Google Gemini (gemini-2.5-flash-lite) initializing them in separate blocks after installing necessary libraries and configuring API keys."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "78b570c3"
      },
      "source": [
        "## Install necessary libraries\n",
        "\n",
        "### Subtask:\n",
        "Install LangChain, LangGraph, and the libraries for interacting with Google Gemini and OpenAI APIs.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "916f144b"
      },
      "source": [
        "**Reasoning**:\n",
        "Install the necessary libraries using pip.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "669aa686",
        "outputId": "4ba841c0-7541-4399-8c41-5230a1ca506e"
      },
      "source": [
        "%pip install --quiet langchain langgraph langchain-google-genai langchain-openai"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/155.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━\u001b[0m \u001b[32m122.9/155.4 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m155.4/155.4 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/50.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.7/50.7 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.0/76.0 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m22.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.2/46.2 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.8/56.8 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m208.3/208.3 kB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-generativeai 0.8.5 requires google-ai-generativelanguage==0.6.15, but you have google-ai-generativelanguage 0.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f9f0aabb"
      },
      "source": [
        "## Configure api keys\n",
        "\n",
        "### Subtask:\n",
        "Add code to securely load and configure API keys for Google Gemini and OpenAI.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c46a1649"
      },
      "source": [
        "**Reasoning**:\n",
        "The subtask is to securely load and configure API keys for Google Gemini and OpenAI. This involves importing `getpass`, prompting the user for the keys, and setting them as environment variables. These steps can be done in a single code block.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "93f6a28c"
      },
      "source": [
        "import getpass\n",
        "import os\n",
        "from google.colab import userdata\n",
        "os.environ['GOOGLE_API_KEY'] = userdata.get('GOOGLE_API_KEY')"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "27fb4665"
      },
      "source": [
        "## Initialize language models\n",
        "\n",
        "### Subtask:\n",
        "Initialize LLMs: from Google (Gemini)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "75590384"
      },
      "source": [
        "**Reasoning**:\n",
        "Import the necessary classes and instantiate the Google Gemini and OpenAI GPT models as specified in the instructions.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "db8ecc57"
      },
      "source": [
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n",
        "gemini_llm = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash-lite\")"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3f687e02"
      },
      "source": [
        "## Define agent tools (optional but recommended)\n",
        "\n",
        "### Subtask:\n",
        "Although not explicitly requested, defining tools for each agent (e.g., knowledge base lookups, external API calls) will enhance their capabilities. This step will be marked as optional in the plan.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7a34c238"
      },
      "source": [
        "**Reasoning**:\n",
        "Define placeholder functions for the tools that each agent might use.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f59d34f0"
      },
      "source": [
        "## Define agent nodes\n",
        "\n",
        "### Subtask:\n",
        "Create a node for each agent (Charging Station, Booking , Cafe, General). Each node will contain the logic for that agent.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b3c32a3f"
      },
      "source": [
        "**Reasoning**:\n",
        "Define the Python functions for each agent (Charging Station, Booking , Cafe, General) as described in the instructions, using the previously initialized language models.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 547
        },
        "id": "721cc284",
        "outputId": "e1bd9374-b90e-4ef8-b62a-a223f09d9faa"
      },
      "source": [
        "## FULL CODE IN THIS BLOCK\n",
        "\n",
        "from langgraph.graph import StateGraph\n",
        "from typing import TypedDict, Annotated, Union\n",
        "import operator\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "from langgraph.graph import StateGraph\n",
        "from typing import TypedDict, Annotated, Union\n",
        "import operator\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "# Define a state for the graph using TypedDict\n",
        "class GraphState(TypedDict):\n",
        "    \"\"\"\n",
        "    Represents the state of our graph.\n",
        "\n",
        "    Attributes:\n",
        "        query: Charging Station Query\n",
        "        classification: classification of the query\n",
        "        chargin_station_enquiry_response: str\n",
        "        booking_response: str\n",
        "        cafe_enquiry_response: str\n",
        "        general_response: str\n",
        "        final_response: str\n",
        "    \"\"\"\n",
        "    query: str\n",
        "    classification: str\n",
        "    chargingStationEnquiry_response: str\n",
        "    booking_response: str\n",
        "    cafeEnquiry_response: str\n",
        "    general_response: str\n",
        "    final_response: str\n",
        "\n",
        "\n",
        "#Redefine the classifier_agent to return a dictionary to update the state\n",
        "def classifier_agent(state: dict) -> dict:\n",
        "    \"\"\"Classifies the customer query and updates the state.\"\"\"\n",
        "    query = state['query']\n",
        "    print(f\"--- Classifier Agent Start ---\")\n",
        "    print(f\"Input Query: {query}\")\n",
        "    prompt = ChatPromptTemplate.from_messages([\n",
        "        (\"system\", \"You are a helpful assistant that classifies customer queries into one of the following categories: booking, charging, cafe, general. Respond with only the category name.\"),\n",
        "        (\"human\", \"Classify the following query: {query}\")\n",
        "    ])\n",
        "    chain = prompt | gemini_llm\n",
        "    category = chain.invoke({\"query\": query}).content.strip().lower() # Ensure clean output\n",
        "    # Basic parsing to extract the category\n",
        "    if \"charging\" in category:\n",
        "        classified_category = \"charging\"\n",
        "    elif \"booking\" in category:\n",
        "        classified_category = \"booking\"\n",
        "    elif \"cafe\" in category:\n",
        "        classified_category = \"cafe\"\n",
        "    else:\n",
        "        classified_category = \"general\"\n",
        "\n",
        "    print(f\"Classified as: {classified_category}\")\n",
        "    print(f\"Output State Update: {{'classification': '{classified_category}'}}\")\n",
        "    print(f\"--- Classifier Agent End ---\")\n",
        "\n",
        "    # Return a dictionary to update the state\n",
        "    return {\"classification\": classified_category}\n",
        "\n",
        "\n",
        "# Redefine the specialized agents to return dictionaries to update the state\n",
        "def chargingStationEnquiry_agent(state: dict) -> dict:\n",
        "    \"\"\"Handles charging station availability queries between 2 locations and updates the state.\"\"\"\n",
        "    query = state['query']\n",
        "    print(f\"--- charging Agent Start ---\")\n",
        "    print(f\"Input Query: {query}\")\n",
        "    prompt = ChatPromptTemplate.from_messages([\n",
        "    (\n",
        "        \"system\",\n",
        "        \"You are an intelligent EV charging assistant. \"\n",
        "        \"Your task is to answer questions about the availability of all electric vehicle charging stations \"\n",
        "        \"between two given locations. Be factual, concise, and user-friendly. \"\n",
        "        \"If possible, mention key routes or highway corridors connecting the two cities.\"\n",
        "    ),\n",
        "    (\n",
        "        \"human\",\n",
        "        \"Query: {query}\\n\\n\"\n",
        "        \"Provide a short, clear response summarizing where charging stations are available \"\n",
        "        \"along the route, and any useful travel tips for EV users.\"\n",
        "    )\n",
        "])\n",
        "    chain = prompt | gemini_llm\n",
        "    response = chain.invoke({\"query\": query}).content\n",
        "    print(f\"Response: {response}\")\n",
        "    print(f\"Output State Update: {{'chargingStationEnquiry_response': '{response}'}}\")\n",
        "    print(f\"--- Charging Agent End ---\")\n",
        "    return {\"chargingStationEnquiry_response\": response}\n",
        "\n",
        "\n",
        "def booking_agent(state: dict) -> dict:\n",
        "    return\n",
        "\n",
        "    # Agent to find cafes nearby to the location shared\n",
        "def cafeEnquiry_agent(state: dict) -> dict:\n",
        "    \"\"\"Consider the locality shared and suggest cafes.\"\"\"\n",
        "    query = state['query']\n",
        "    print(f\"--- Cafe Agent Start ---\")\n",
        "    print(f\"Input Query: {query}\")\n",
        "    prompt = ChatPromptTemplate.from_messages([\n",
        "        (\"system\", \"You are a helpful assistant that can suggest cafes based on location described in {query}.\"\n",
        "        \"Consdier only those cafes which have EV charging station available with it.\"\n",
        "        \"Consider only the cafes which are open and have review rating of 4 and above.\"\n",
        "        ),\n",
        "        (\"human\", \"Respond to the following cafe related query: {query}\")\n",
        "    ])\n",
        "    chain = prompt | gemini_llm\n",
        "    response = chain.invoke({\"query\": query}).content\n",
        "    print(f\"Response: {response}\")\n",
        "    print(f\"Output State Update: {{'cafe_enquiry_response': '{response}'}}\")\n",
        "    print(f\"--- Cafe Agent End ---\")\n",
        "    return {\"cafe_enquiry_response\": response}\n",
        "\n",
        "def general_agent(state: dict) -> dict:\n",
        "    \"\"\"Handles general queries and updates the state.\"\"\"\n",
        "    query = state['query']\n",
        "    print(f\"--- General Agent Start ---\")\n",
        "    print(f\"Input Query: {query}\")\n",
        "    prompt = ChatPromptTemplate.from_messages([\n",
        "        (\"system\", \"You are a helpful assistant that responds to general queries.\"),\n",
        "        (\"human\", \"Respond to the following general query: {query}\")\n",
        "    ])\n",
        "    chain = prompt | gemini_llm\n",
        "    response = chain.invoke({\"query\": query}).content\n",
        "    print(f\"Response: {response}\")\n",
        "    print(f\"Output State Update: {{'general_response': '{response}'}}\")\n",
        "    print(f\"--- General Agent End ---\")\n",
        "    return {\"general_response\": response}\n",
        "\n",
        "# Redefine the router node\n",
        "def router(state: dict) -> str:\n",
        "    \"\"\"Routes the query based on the classifier's output.\"\"\"\n",
        "    classification = state.get('classification')\n",
        "    print(f\"--- Router Start ---\")\n",
        "    print(f\"Current state: {state}\")\n",
        "    print(f\"Received classification: {classification}\")\n",
        "\n",
        "    if classification == \"charging\":\n",
        "        next_node = \"chargingStationEnquiry_agent\"\n",
        "    elif classification == \"booking\":\n",
        "        next_node = \"booking_agent\"\n",
        "    elif classification == \"cafe\":\n",
        "        next_node = \"cafeEnquiry_agent\"\n",
        "    else:\n",
        "        # Fallback to general agent for any other classification\n",
        "        next_node = \"general_agent\"\n",
        "\n",
        "    print(f\"Routing to: {next_node}\")\n",
        "    print(f\"--- Router End ---\")\n",
        "    return next_node\n",
        "\n",
        "# Redefine the aggregate_response node\n",
        "def aggregate_response(state: dict) -> dict:\n",
        "    \"\"\"Collects responses from specialized agents and formats a single response.\"\"\"\n",
        "    print(f\"--- Aggregation Node ---\")\n",
        "    print(f\"Current state before aggregation: {state}\")\n",
        "\n",
        "    chargingStationEnquiry_response = state.get('chargingStationEnquiry_response', \"\")\n",
        "    booking_response = state.get('booking_response', \"\")\n",
        "    cafeEnquiry_response = state.get('cafeEnquiry_response', \"\")\n",
        "    general_response = state.get('general_response', \"\")\n",
        "\n",
        "    responses = []\n",
        "    if chargingStationEnquiry_response:\n",
        "        responses.append(f\"charging Station Enquiry Response: {chargingStationEnquiry_response}\")\n",
        "    if booking_response:\n",
        "        responses.append(f\"booking Response: {booking_response}\")\n",
        "    if cafeEnquiry_response:\n",
        "        responses.append(f\"cafe Enquiry Response: {cafeEnquiry_response}\")\n",
        "    if general_response:\n",
        "        responses.append(f\"General Response: {general_response}\")\n",
        "\n",
        "    final_response = \"\\n\".join(responses)\n",
        "    # Update the state with the final response.\n",
        "    # Ensure this is a dictionary update as expected by StateGraph.\n",
        "    update_dict = {'final_response': final_response}\n",
        "\n",
        "    print(f\"Aggregated Final Response: {final_response}\")\n",
        "    print(f\"Output State Update: {update_dict}\")\n",
        "    print(f\"------------------------\")\n",
        "    return update_dict\n",
        "\n",
        "\n",
        "# Re-compile the graph with the updated nodes\n",
        "# Instantiate a StateGraph with the defined state\n",
        "workflow = StateGraph(GraphState)\n",
        "\n",
        "# Add the updated agent nodes and the router node\n",
        "workflow.add_node(\"classifier_agent\", classifier_agent)\n",
        "workflow.add_node(\"chargingStationEnquiry_agent\", chargingStationEnquiry_agent)\n",
        "workflow.add_node(\"booking_agent\", booking_agent)\n",
        "workflow.add_node(\"cafeEnquiry_agent\", cafeEnquiry_agent)\n",
        "workflow.add_node(\"general_agent\", general_agent)\n",
        "workflow.add_node(\"router\", router)\n",
        "workflow.add_node(\"aggregate_response\", aggregate_response)\n",
        "\n",
        "\n",
        "# Set the entry point of the graph\n",
        "workflow.set_entry_point(\"classifier_agent\")\n",
        "# workflow.add_edge(\"classifier_agent\", \"router\")\n",
        "\n",
        "# Instead, use add_conditional_edges from classifier_agent:\n",
        "workflow.add_conditional_edges(\n",
        "    \"classifier_agent\",  # source node\n",
        "    router,              # routing function\n",
        "    {\n",
        "        \"charging\": \"chargingStationEnquiry_agent\",\n",
        "        \"booking\": \"booking_agent\",\n",
        "        \"cafe\": \"cafeEnquiry_agent\",\n",
        "        \"general\": \"general_agent\"\n",
        "    }\n",
        ")\n",
        "\n",
        "# REMOVE explicit edges from specialized agents to aggregate_response\n",
        "# workflow.add_edge(\"billing_agent\", \"aggregate_response\")\n",
        "# workflow.add_edge(\"technical_agent\", \"aggregate_response\")\n",
        "# workflow.add_edge(\"general_agent\", \"aggregate_response\")\n",
        "\n",
        "# Set the aggregate_response node as the end point of the graph\n",
        "# We will now add edges from the specialized agents to the aggregate_response node\n",
        "workflow.add_edge(\"chargingStationEnquiry_agent\", \"aggregate_response\")\n",
        "workflow.add_edge(\"booking_agent\", \"aggregate_response\")\n",
        "workflow.add_edge(\"cafeEnquiry_agent\", \"aggregate_response\")\n",
        "workflow.add_edge(\"general_agent\", \"aggregate_response\")\n",
        "\n",
        "workflow.set_finish_point(\"aggregate_response\")\n",
        "\n",
        "\n",
        "# Compile the graph\n",
        "app = workflow.compile()\n",
        "\n",
        "# Define sample customer queries\n",
        "sample_queries = [\n",
        "    \"I am travelling from Bangalore to Pune find a charging station on my way?\",  # charging station  query\n",
        "]\n",
        "\n",
        "# Iterate through sample queries and test the workflow\n",
        "for query in sample_queries:\n",
        "    print(f\"=== Running workflow for query: {query} ===\")\n",
        "    # Invoke the compiled LangGraph application\n",
        "    # The initial state should contain the input query\n",
        "    result = app.invoke({\"query\": query})\n",
        "    # The final response is stored in the 'final_response' key of the final state\n",
        "    print(f\"--- Final Result ---\")\n",
        "    print(f\"Input Query: {query}\")\n",
        "    print(f\"Final Response: {result.get('final_response', 'No final response generated.')}\")\n",
        "    print(f\"===========================================\\n\")"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Running workflow for query: I am travelling from Bangalore to Pune find a charging station on my way? ===\n",
            "--- Classifier Agent Start ---\n",
            "Input Query: I am travelling from Bangalore to Pune find a charging station on my way?\n",
            "Classified as: charging\n",
            "Output State Update: {'classification': 'charging'}\n",
            "--- Classifier Agent End ---\n",
            "--- Router Start ---\n",
            "Current state: {'classification': 'charging', 'query': 'I am travelling from Bangalore to Pune find a charging station on my way?'}\n",
            "Received classification: charging\n",
            "Routing to: chargingStationEnquiry_agent\n",
            "--- Router End ---\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "'chargingStationEnquiry_agent'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3182790530.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    244\u001b[0m     \u001b[0;31m# Invoke the compiled LangGraph application\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    245\u001b[0m     \u001b[0;31m# The initial state should contain the input query\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 246\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mapp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"query\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mquery\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    247\u001b[0m     \u001b[0;31m# The final response is stored in the 'final_response' key of the final state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    248\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"--- Final Result ---\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langgraph/pregel/main.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config, context, stream_mode, print_mode, output_keys, interrupt_before, interrupt_after, durability, **kwargs)\u001b[0m\n\u001b[1;32m   3092\u001b[0m         \u001b[0minterrupts\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mInterrupt\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3093\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3094\u001b[0;31m         for chunk in self.stream(\n\u001b[0m\u001b[1;32m   3095\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3096\u001b[0m             \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langgraph/pregel/main.py\u001b[0m in \u001b[0;36mstream\u001b[0;34m(self, input, config, context, stream_mode, print_mode, output_keys, interrupt_before, interrupt_after, durability, subgraphs, debug, **kwargs)\u001b[0m\n\u001b[1;32m   2677\u001b[0m                     \u001b[0;32mfor\u001b[0m \u001b[0mtask\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mloop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatch_cached_writes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2678\u001b[0m                         \u001b[0mloop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_writes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrites\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcached\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2679\u001b[0;31m                     for _ in runner.tick(\n\u001b[0m\u001b[1;32m   2680\u001b[0m                         \u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mloop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrites\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2681\u001b[0m                         \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep_timeout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langgraph/pregel/_runner.py\u001b[0m in \u001b[0;36mtick\u001b[0;34m(self, tasks, reraise, timeout, retry_policy, get_waiter, schedule_task)\u001b[0m\n\u001b[1;32m    165\u001b[0m             \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtasks\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 167\u001b[0;31m                 run_with_retry(\n\u001b[0m\u001b[1;32m    168\u001b[0m                     \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m                     \u001b[0mretry_policy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langgraph/pregel/_retry.py\u001b[0m in \u001b[0;36mrun_with_retry\u001b[0;34m(task, retry_policy, configurable)\u001b[0m\n\u001b[1;32m     40\u001b[0m             \u001b[0mtask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrites\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m             \u001b[0;31m# run the task\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mtask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mParentCommand\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m             \u001b[0mns\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mCONF\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mCONFIG_KEY_CHECKPOINT_NS\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langgraph/_internal/_runnable.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    656\u001b[0m                         \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    657\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 658\u001b[0;31m                     \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    659\u001b[0m         \u001b[0;31m# finish the root run\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    660\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langgraph/_internal/_runnable.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    398\u001b[0m                 \u001b[0mrun_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_chain_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    399\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 400\u001b[0;31m             \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    401\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecurse\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRunnable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    402\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langgraph/graph/_branch.py\u001b[0m in \u001b[0;36m_route\u001b[0;34m(self, input, config, reader, writer)\u001b[0m\n\u001b[1;32m    165\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 167\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_finish\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwriter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m     async def _aroute(\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langgraph/graph/_branch.py\u001b[0m in \u001b[0;36m_finish\u001b[0;34m(self, writer, input, result, config)\u001b[0m\n\u001b[1;32m    201\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mends\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m             destinations: Sequence[Send | str] = [\n\u001b[0;32m--> 203\u001b[0;31m                 \u001b[0mr\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSend\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mends\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    204\u001b[0m             ]\n\u001b[1;32m    205\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'chargingStationEnquiry_agent'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "32s5cGc3EibQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}